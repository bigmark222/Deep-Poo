# Current weak points / risks

- Model maturity: TinyDet is minimal, uses host-side CIoU, limited augmentation ✅(flip/letterbox), and no robust target assignment or mAP computation.
  - Next steps:
    - Tensorize CIoU in the loss ✅
    - Add stronger augmentations ✅ (color jitter added; consider scale/crop jitter next)
    - Improve target assignment ✅
    - Add mAP/PR metrics in validation ✅
- Data quality: synthetic-only captures; limited augmentation diversity.
  - Track metrics on synthetic val to guide aug/realism tweaks.✅
  - Expand aug: stronger color jitter, blur/noise, scale/crop with bbox-safe transforms.✅
  - Increase synthetic realism: lighting/texture noise/camera jitter sweeps; see if metrics improve.✅
  - Enforce split hygiene: seeded shuffles and stratified sampling to balance box sizes/classes across train/val.✅
- Checkpointing & reproducibility: no bundled demo weights; splits not persisted (seeded shuffles exist but not enforced across runs).
  - Persist splits: save/load a split manifest (train/val indices or run IDs) so reruns reuse the same split. ✅ (`--split-manifest`)
  - Enforce seeds: default to a fixed seed or log the seed used per run for repeatability. ✅ (default seed logged)
  - Bundle a demo checkpoint and document how to load it (or a CLI flag to download/load if present). ✅ (flag `--demo-checkpoint` added; need actual weights)
- Validation metrics: only mean IoU + precision/recall; no mAP/PR curves; NMS/threshold assumptions may not match deployment.
  - Add proper PR/mAP computation across thresholds (beyond the current approximate sweep). ✅ (objectness sweep 0.05..0.95 + multiple IoU thresholds)
  - Expose evaluation thresholds (obj/IoU) to align with deployment and allow multi-threshold eval. ✅ (`--val-iou-sweep`)
  - Persist val metrics per epoch (JSON/CSV) for tracking. ✅ (`--metrics-out` JSONL)
  - Add an eval-only command to score a checkpoint on a dataset without training. ✅ (`bin/eval`)
- Runtime UX: HUD overlays are basic; thresholds require flags (no live toggle); heuristic fallback only logged.
  - Improve HUD: draw boxes with score labels/counts and better styling. ✅
  - Live threshold controls: add runtime UI/keys to adjust obj/IoU thresholds (not just CLI). ✅ (keys -/= for obj, [/] for IoU; HUD shows updates)
  - Detector toggle: runtime switch between Burn and heuristic detectors with on-screen indicator. ✅ (key `B`, HUD shows mode)
  - Fallback messaging: visible HUD banner/toast when Burn model missing/fails (in addition to logs). ✅
- Performance: CPU NdArray backend and CPU NMS; no batching/GPU path, potential real-time bottleneck.
  - Add a GPU-backed Burn backend (e.g., tch/wgpu) and use it when available for train/inference. ✅ (wgpu backend behind `burn_wgpu`)
  - Optimize NMS (SIMD/parallel or GPU) and reduce clones/allocs. ✅ (in-place indices, no per-call clones)
  - Support batch > 1 via target assignment/loader to improve throughput. ✅ (multi-image target build; batch_size flag enabled)
  - Add a simple timing overlay/log for inference/frame processing. ✅ (HUD shows inference ms)
- Testing gaps: no end-to-end inference test in the sim; no HUD overlay tests; Burn training harness lacks a sanity test beyond a single-batch run.
  - Add an end-to-end sim inference test that drives a few frames and asserts detections/HUD updates. ✅ (tests/vision_inference.rs)
  - Add HUD overlay UI snapshot/logic tests (boxes, scores, fallback banner). ✅ (tests/hud_overlay.rs)
  - Add a Burn training harness sanity test (multi-step, batch>1) under `burn_runtime` to catch API drift. ✅ (tests/train_harness.rs)
- Docs: training doc lacks full workflow (data prep, expected outputs, sample checkpoint); deployment guidance for Burn model is minimal.
  - Expand `training.md` with an end-to-end workflow: data prep (capture/prune), expected outputs/artifacts, and a sample command. ✅
  - Document how to obtain/use a sample checkpoint (download path or bundled file) and how to load it via CLI flag. ✅ (training.md)
  - Add deployment guidance: where to place the Burn checkpoint for runtime, how to toggle detectors, and inference threshold knobs. ✅ (training.md, cli.md)
